---
title: "Probability Distributions and Functions"
subtitle: POSC 3410  -- Quantitative Methods in Political Science
author: Steven V. Miller
institute: Department of Political Science
titlegraphic: /Dropbox/teaching/clemson-academic.png
date: 
output:
 beamer_presentation:
    template: ~/Dropbox/miscelanea/svm-r-markdown-templates/svm-latex-beamer.tex
    latex_engine: xelatex
    dev: cairo_pdf
    fig_caption: true
    slide_level: 3
make149: true
mainfont: "Open Sans"
titlefont: "Titillium Web"
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
---

```{r setup, include=FALSE, cache=F, message=F, warning=F, results="hide"}
knitr::opts_chunk$set(cache=TRUE, message=F, warning=FALSE)
knitr::opts_chunk$set(fig.path='figs/')
knitr::opts_chunk$set(cache.path='cache/')

knitr::opts_chunk$set(
                  fig.process = function(x) {
                      x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      if (file.rename(x, x2)) x2 else x
                      }
                  )
```

```{r loadpackages, include=FALSE, cache=FALSE}
library(tidyverse)
library(stevemisc)
library(knitr)
library(kableExtra)

parab <- function(x) {-x^2/2}
expparab <- function(x) {exp(-x^2/2)}

```


# Introduction
### Goal for Today

*Discuss probability distributions*.

### Introduction

Last lecture discussed probability and counting.

- While abstract, these are important foundation concepts for what we're doing in applied statistics.

Today, we're going to talk about probability distributions.

- Our most prominent tool for statistical inference makes assumptions about parameters given a known (i.e. normal) distribution.

### Refresher

Recall the choose notation (aka **combination**):

\begin{equation}
  {n \choose k} = \frac{n!}{(n-k)!k!}
\end{equation}

\bigskip The exclamation marks indicate a factorial.

- e.g. 5! = 5 * 4 * 3 * 2 * 1.

# Binomial Functions
## Binomial Theorem
### Binomial Theorem

The most common use of a choose notation is the **binomial theorem**.

- Given any real numbers *X* and *Y* and a nonnegative integer *n*,

\begin{equation}
  (X + Y)^n = \sum \limits_{k=0}^n {n \choose k} x^k y^{n-k}
\end{equation}

\bigskip

A special case occurs when *X* = 1 and *Y* = 1.

\begin{equation}
  2^n = \sum \limits_{k=0}^n {n \choose k}
\end{equation}

### Binomial Theorem

This is another theorem with an interesting history.

- Euclid knew of it in a simple form.
- The Chinese may have discovered it first (Chu Shi-KiÃ©, 1303)
- General form presented here owes to Pascal in 1654.

### Binomial Theorem

The binomial expansion increases in polynomial terms at an interesting rate.

\begin{eqnarray}
(X + Y)^0 &=& 1 \nonumber \\
(X + Y)^1 &=& X + Y \nonumber \\
(X + Y)^2 &=& X^2 + 2XY + Y^2 \nonumber \\
(X + Y)^3 &=& X^3 + 3X^2Y + 3XY^2 + Y^3 \nonumber \\
(X + Y)^4 &=& X^4 + 4X^3Y + 6X^2Y^2 + 4XY^3 + Y^4 \nonumber \\
(X + Y)^5 &=& X^5 + 5X^4Y + 10X^3Y^2 + 10X^2Y^3 + 5XY^4 + Y^5 
\end{eqnarray}

Notice the symmetry?

## Pascal's Triangle
### Pascal's Triangle

The coefficients form **Pascal's triangle**, which summarizes the coefficients in a binomial expansion.


\begin{tabular}{cccccccccccccc}
$n=0$:& & & &    &    &    &    &  1\\\noalign{\smallskip\smallskip}
$n=1$:& & & &   &    &    &  1 &    &  1\\\noalign{\smallskip\smallskip}
$n=2$:& & & &   &    &  1 &    &  2 &    &  1\\\noalign{\smallskip\smallskip}
$n=3$:& & & &   &  1 &    &  3 &    &  3 &    &  1\\\noalign{\smallskip\smallskip}
$n=4$:& & & & 1 &    &  4 &    &  6 &    &  4 &    &  1\\\noalign{\smallskip\smallskip}
$n=5$: & & & 1 &   &  5  &   &  10  &   &  10  &   &  5  &  & 1\\\noalign{\smallskip\smallskip}
\end{tabular}

### Pascal's Triangle

Beyond the pyramidal symmetry, Pascal's triangle has a lot other cool features.

- Any value in the table is the sum of the two values diagonally above it.
- The sum of the *k*th row (counting the first row as zero row) can be calculated as $\sum\limits_{j=0}^k {k \choose j} = 2^k$
- If you left-justify the triangle, the sum of the diagonals form a Fibonacci sequence.
- If a row is treated as consecutive digits, each row is a power of 11 (i.e. magic 11s).

There are many more mathematical properties in Pascal's triangle. These are just the cooler/more famous ones.

## Binomial Mass Function
### These Have a Purpose for Statistics

Let's start basic: how many times could we get heads in 10 coin flips?

- The sample space *S* = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 }
- We expect 10 heads (or no heads) to be unlikely, assuming the coin is fair.

### Binomial Mass Function

*This is a combination issue*.

- For no heads, *every* flip must be a tail.
- For just one head, we have more combinations.

How many ways can a series of coin flips land on just one head?

- For a small number of trials, look at Pascal's triangle.
- For 5 trials, there is 1 way to obtain 0 heads, 5 ways to obtain 1 head, 10 ways to obtain 2 and 3 heads, 5 ways to obtain 4 heads, and 1 way to obtain 5 heads.

### Binomial Mass Function

This is also answerable by reference to the **binomial mass function**, itself derivative of the **binomial theorem**.

\begin{equation}
  p(x) = {n \choose x} p^x(1-p)^{n - x}, 
\end{equation}

where:

- *x* = the count of "successes" (e.g. number of heads in a sequence of coin flips)
- *n* = the number of trials.
- *p* = probability of success in any given trial.

### Binomial Mass Function

What's the probability of getting five heads on ten fair coin flips.

\begin{eqnarray}
p(x = 5 \thinspace | \thinspace n = 10, p = .5) &=& {10 \choose 5 } (.5)^5(1-.5)^{10-5} \nonumber \\
&=& (252)*(.03125)*(.03125) \nonumber \\
 &=& 0.2460938
\end{eqnarray}


In R:

```{r}
dbinom (5,10,.5)
```

### An Application: Everyone Hates Congress

Congress is doing nothing at a historic rate. This much we know.

- About 5\% of bills that receive "some action" are ultimately passed.
    - Recall: many bills introduced die a quick death from inactivity.
    - This estimate says nothing about substantive importance of the bill.
  

Assume *p* = .05. What's the probability that Congresses passes three (*x*) of the next 20 (*n*) bills it gets?

### An Application: Everyone Hates Congress



\begin{eqnarray}
p(x = 3 \thinspace | \thinspace n = 20, p = .05) &=& {20 \choose 3 } (.05)^3(1-.05)^{20-3} \nonumber \\
&=& (1140)*(.000125)*(0.4181203) \nonumber \\
 &=& 0.05958215
\end{eqnarray}


In R:

```{r}
dbinom(3,20,.05)
```


# Normal Functions
## Normal Density Function
### Normal Functions

A "normal" function is also quite common.

- Data are distributed such that the majority cluster around some central tendency.
- More extreme cases occur less frequently.

### Normal Density Function

We can model this with a **normal density function**.

- Sometimes called a Gaussian distribution in honor of Carl Friedrich Gauss, who discovered it.

\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e \thinspace \{ -\frac{(x -\mu)^2}{2\sigma^2} \},
\end{equation}

where: $\mu$ = the mean, $\sigma^2$ = the variance.

### Normal Density Function

Properties of the normal density function.

- The tails are asymptote to 0.
- The kernel (inside the exponent) is a basic parabola.
    - The negative component flips the parabola downward.
- Denoted as a function in lieu of a probability because it is a continuous distribution.
- The distribution is perfectly symmetrical.
    - The mode/median/mean are the same values.
    - *-x* is as far from $\mu$ as *x*.
    
### Normal Density Function

*x* is unrestricted. It can be any value you want in the distribution.

- $\mu$ and $\sigma^2$ are parameters that define the shape of the distribution.
    - $\mu$ defines the central tendency. 
    - $\sigma^2$ defines how short/wide the distribution is.
    

## Demystifying the Normal Distribution    
### Demystifying the Normal Density Function

Let's unpack this normal density function further (and use some R code).

### Demystifying the Normal Density Function

Here is our normal density function.

\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e \thinspace \{ -\frac{(x -\mu)^2}{2\sigma^2} \}
\end{equation}

Assume, for simplicity, $\mu$ = 0 and $\sigma^2$ = 1.

### Demystifying the Normal Density Function

When $\mu$ = 0 and $\sigma^2$ = 1, the normal density function is a bit simpler.

\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi}}e \thinspace \{ -\frac{x^2}{2} \}
\end{equation}

Let's plot it next in R.

```{r shownormald, echo=TRUE, eval=F}
ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  theme_steve_web() + # from stevemisc
  stat_function(fun = dnorm, color="#522D80", size=1.5) 
```

### 

```{r normald, eval=TRUE, echo=FALSE,  fig.width=14, fig.height=8.5, warning=F, message=F}
ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  theme_steve_web() + # from stevemisc
  stat_function(fun = dnorm, color="#522D80", size=1.5) +
  labs(title = "A Simple Normal Density Function",
       subtitle = "The mu parameter determines the central tendency and sigma-squared parameter determines the width.",
       x = "", y="")
```

### Demystifying the Normal Distribution

Let's look inside the exponent.

- The term inside the brackets (-$x^2$/2) is a parabola.
- Exponentiating it makes it asymptote to 0.

### R Code

```{r showexpcode, eval=F, echo=T}
library(ggplot2)
parab <- function(x) {-x^2/2}
expparab <- function(x) {exp(-x^2/2)}

ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = parab, color="#522d80", size=1.5) +
  theme_steve_web() 

ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = expparab, color="#522d80", size=1.5) +
  theme_steve_web() 
```


### 

```{r parab, eval=TRUE, echo=FALSE,  fig.width=14, fig.height=8.5, warning=F, message=F}
ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = parab, color="#522d80", size=1.5) +
  theme_steve_web()  +
  labs(title="A Basic Parabola",
       subtitle = "Notice the height is at 0 because the negative part flipped the parabola downward.",
       x = "", y="") 
```

### 

```{r expparab, echo=F, eval=T}
ggplot(data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = expparab, color="#522d80", size=1.5) +
  theme_steve_web() +
  labs(title="An Exponentiated Negative Parabola",
       subtitle = "Exponentiating squeezes the parabola, adjusts the height, and makes the tails asymptote to 0.",
       x = "", y="") 
```

### Demystifying the Normal Distribution

When the numerator in the brackets is zero (i.e. $x = \mu$, here: 0), this devolves to an exponent of 0.

- *exp*(0) = 1 (and, inversely, *log*(1) = 0).
- A logarithm of *x* for some base *b* is the value of the exponent that gets *b* to *x*.
    - $log_b(x) = a \quad =\Rightarrow \quad b^a = x$
- Notice how the top of the curve was at 1 in the exponentiated parabola.

### Demystifying the Normal Density Function

With that in mind, it should be clear that $\frac{1}{\sqrt{2\pi\sigma^2}}$ (recall: $\sigma^2 = 1$ in our simple case) determines the height of the distribution.

### Demystifying the Normal Density Function

Observe: 

```{r sqrt}
1/sqrt(2*pi)

dnorm(0,mean=0,sd=1)

```

The height of the distribution for $x = 0$ when $\mu = 0$ and $\sigma^2 = 1$ is .3989423.

### Demystifying the Normal Distribution

Notice: we talked about the height and shape of the distribution as a $function$. It does not communicate probabilities.

- The normal distribution is continuous. Thus, probability for any one value is basically 0.

That said, the area *under* the curve is the full domain and equals 1.

- The probability of selecting a number between two points on the x-axis equals the area under the curve *between* those two points.

### Demystifying the Normal Density Function

Observe:

```{r area}
pnorm(0, mean=0, sd=1)

```

### Demystifying the Normal Distribution

\small

```{r showshadehalf, echo=T, eval=F}
ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  theme_steve_web() +
  stat_function(fun = dnorm, 
                xlim = c(-4,0),
                size=0,
                geom = "area", fill="#F66733", alpha=.5) + 
    stat_function(fun = dnorm, color="#522D80", size=1.5) 

```

\normalsize

### 

```{r shadehalf, eval=TRUE, echo=FALSE,  fig.width=14, fig.height=8.5, warning=F, message=F}
ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  theme_steve_web() +
  labs(caption="-Infinity to 0 has 50% of the area under the curve") + 
  stat_function(fun = dnorm, 
                xlim = c(-4,0),
                size=0,
                geom = "area", fill="#F66733", alpha=.5) + 
    stat_function(fun = dnorm, color="#522D80", size=1.5) +
  labs(title = "A Standard Normal Distribution",
       subtitle = "Notice that half the distribution lies between negative infinity and 0.")
```


### 68-90-95-99

```{r thenodes}
pnorm(1,mean=0,sd=1)-pnorm(-1,mean=0,sd=1)
pnorm(1.645,mean=0,sd=1)-pnorm(-1.645,mean=0,sd=1)
pnorm(1.96,mean=0,sd=1)-pnorm(-1.96,mean=0,sd=1)
pnorm(2.58,mean=0,sd=1)-pnorm(-2.58,mean=0,sd=1)
```


###


```{r ggplotshade, eval=TRUE, echo=FALSE,  fig.width=14, fig.height=8.5, warning=F, message=F}
normal_dist("#522d80","#F66733", "Open Sans") + 
  theme_steve_web() + 
  # ^ all from stevemisc
    labs(title = "The Area Underneath a Normal Distribution",
       subtitle = "The tails extend to infinity and are asymptote to zero, but the full domain sums to 1. 95% of all possible values are within about 1.96 standard units from the mean.",
       y = "Density",
       x = "")
```

# Conclusion
### Conclusion

There are a lot of topics to digest in this lecture, all worth knowing.

- Probability and probability distributions are core components of the inferential statistics we'll be doing next.
